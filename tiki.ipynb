{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_filename(title):\n",
    "    # Remove any illegal characters from the title\n",
    "    safe_title = \"\".join(c for c in title if c.isalnum() or c in [\" \", \"_\", \"-\"])\n",
    "\n",
    "    # Encode the title using MD5\n",
    "    encoded_title = hashlib.md5(safe_title.encode()).hexdigest()\n",
    "\n",
    "    return encoded_title\n",
    "\n",
    "\n",
    "def extract_info(soup):\n",
    "    try:\n",
    "        # Extract title\n",
    "        title = soup.find(\n",
    "            \"h1\", class_=\"Title__TitledStyled-sc-c64ni5-0 iXccQY\"\n",
    "        ).text.strip()\n",
    "\n",
    "        # Extract author's name\n",
    "        author_tag = soup.find(\"a\", {\"data-view-id\": \"pdp_details_view_author\"})\n",
    "        author_name = author_tag.text.strip() if author_tag else None\n",
    "        return {\"title\": title, \"author\": author_name}\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def extract_rating(soup):\n",
    "    try:\n",
    "        rating_summary = soup.find(\"div\", class_=\"review-rating__inner\")\n",
    "        total_rating = float(rating_summary.find(\"div\", class_=\"review-rating__point\").get_text(strip=True))\n",
    "        number_of_ratings_text = rating_summary.find(\"div\", class_=\"review-rating__total\").get_text(strip=True)\n",
    "        number_of_ratings = int(''.join(filter(str.isdigit, number_of_ratings_text)))\n",
    "\n",
    "        return {\n",
    "            \"total_rating\": total_rating,\n",
    "            \"number_of_rating\": number_of_ratings\n",
    "        }\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def extract_reviews(soup):\n",
    "    try:\n",
    "        reviews = soup.find_all(\n",
    "            \"div\", class_=\"style__StyledComment-sc-1y8vww-5 dpVjwc review-comment\"\n",
    "        )\n",
    "        review_data = []\n",
    "        for review in reviews:\n",
    "            # Extract user information\n",
    "            user_name = review.find(\"div\", class_=\"review-comment__user-name\").get_text(\n",
    "                strip=True\n",
    "            )\n",
    "            user_date = review.find(\"div\", class_=\"review-comment__user-date\").get_text(\n",
    "                strip=True\n",
    "            )\n",
    "\n",
    "            # Extract review count and liked count\n",
    "            user_info_divs = review.find_all(\"div\", class_=\"review-comment__user-info\")\n",
    "            review_count = user_info_divs[0].find(\"span\").get_text(strip=True)\n",
    "            liked_count = user_info_divs[1].find(\"span\").get_text(strip=True)\n",
    "\n",
    "            # Extract review title\n",
    "            review_title = review.find(\"div\", class_=\"review-comment__title\").get_text(\n",
    "                strip=True\n",
    "            )\n",
    "\n",
    "            # Determine rating based on review title\n",
    "            if review_title == \"Cực kì hài lòng\":\n",
    "                rating = 5\n",
    "            elif review_title == \"Hài lòng\":\n",
    "                rating = 4\n",
    "            elif review_title == \"Bình thường\":\n",
    "                rating = 3\n",
    "            elif review_title == \"Không hài lòng\":\n",
    "                rating = 2\n",
    "            else:\n",
    "                rating = 1\n",
    "\n",
    "            # Extract review content\n",
    "            review_content = review.find(\"div\", class_=\"review-comment__content\").get_text(\n",
    "                strip=True\n",
    "            )\n",
    "\n",
    "            review_data.append(\n",
    "                {\n",
    "                    \"reviewer\": {\n",
    "                        \"name\": user_name,\n",
    "                        \"date\": user_date,\n",
    "                        \"review_count\": review_count,\n",
    "                        \"liked_count\": liked_count,\n",
    "                    },\n",
    "                    \"review\": {\n",
    "                        \"rating\": rating,\n",
    "                        \"title\": review_title,\n",
    "                        \"content\": review_content,\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "        return review_data\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def extract_details(soup):\n",
    "    try:\n",
    "        details = {}\n",
    "        # Find all divs with class 'WidgetTitle__WidgetContentStyled-sc-12sadap-2'\n",
    "        content_divs = soup.find_all(\n",
    "            \"div\", class_=\"WidgetTitle__WidgetContentStyled-sc-12sadap-2 hNNYbU\"\n",
    "        )\n",
    "\n",
    "\n",
    "        for div in content_divs:\n",
    "            # Get the label and value spans\n",
    "            label_span = div.find(\n",
    "                \"span\", style=\"max-width: 300px; color: rgb(128, 128, 137);\"\n",
    "            )\n",
    "            value_span = div.find(\n",
    "                \"span\", class_=\"styles__ProductAttributeValueStyled-sc-vjutbk-0 chhHdv\"\n",
    "            )\n",
    "\n",
    "            if label_span and value_span:\n",
    "                # Extract label and value text\n",
    "                label = label_span.text.strip()\n",
    "                value = value_span.text.strip()\n",
    "\n",
    "                # Store label and value in details dictionary\n",
    "                details[label] = value\n",
    "\n",
    "        return details\n",
    "    except:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"https://tiki.vn/tam-ly-gioi-tinh/c868\",\n",
    "    \"https://tiki.vn/sach-ky-nang-song/c870\",\n",
    "   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_books_from_category(base_dir, url):\n",
    "    # Extract the category name from the URL\n",
    "    category_name = url.split(\"/\")[-2]\n",
    "\n",
    "    # Create a directory for the category\n",
    "    category_dir = os.path.join(base_dir, category_name)\n",
    "    if not os.path.exists(category_dir):\n",
    "        os.makedirs(category_dir)\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    ActionChains(driver).scroll_by_amount(0, 800).perform()\n",
    "\n",
    "    # Wait until the presence of at least one element with data-view-id=\"product_list_item\"\n",
    "    wait = WebDriverWait(driver, 15)\n",
    "    a_tags = wait.until(\n",
    "        EC.presence_of_all_elements_located(\n",
    "            (By.CSS_SELECTOR, 'a[data-view-id=\"product_list_item\"]')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Now you can proceed to extract data from the a_tags\n",
    "    # For example:\n",
    "    urls = []\n",
    "    for a_tag in a_tags:\n",
    "        href = a_tag.get_attribute(\"href\")\n",
    "        urls.append(href)\n",
    "\n",
    "    for url in urls:\n",
    "        # Load the webpage\n",
    "        driver.get(url)\n",
    "        # Wait for 3 seconds\n",
    "        time.sleep(3)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        info_data = extract_info(soup)\n",
    "        # Get the title\n",
    "        title = info_data[\"title\"]\n",
    "        filename = os.path.join(category_dir, f\"{encode_filename(title)}.json\")\n",
    "        if os.path.exists(filename):\n",
    "            continue\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Scroll to the details section\n",
    "        ActionChains(driver).scroll_by_amount(0, 2000).perform()\n",
    "        time.sleep(5)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        details = extract_details(soup)\n",
    "\n",
    "        # Scroll to the desired section\n",
    "        ActionChains(driver).scroll_by_amount(0, 1300).perform()\n",
    "        time.sleep(5)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        rating = extract_rating(soup)\n",
    "        review_data = []\n",
    "\n",
    "        try:\n",
    "            btn_next = driver.find_element(\n",
    "                By.CSS_SELECTOR,\n",
    "                \"#customer-review-widget-id > div > div.style__StyledCustomerReviews-sc-1y8vww-0.gCaHEu.customer-reviews > div > div.customer-reviews__pagination > ul > li:nth-child(7) > a\",\n",
    "            )\n",
    "\n",
    "            while btn_next:\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                data = extract_reviews(soup)\n",
    "                if data == []:\n",
    "                    break\n",
    "                review_data.extend(data)\n",
    "                actions = ActionChains(driver)\n",
    "                actions.move_to_element(btn_next)\n",
    "                actions.click(btn_next)\n",
    "                actions.perform()\n",
    "                time.sleep(3)\n",
    "        except:\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            data = extract_reviews(soup)\n",
    "            review_data.extend(data)\n",
    "        finally:\n",
    "            # Generate filename\n",
    "            filename = os.path.join(category_dir, f\"{encode_filename(title)}.json\")\n",
    "            data = {\n",
    "                \"info\": info_data,\n",
    "                \"rating\": rating,\n",
    "                \"details\": details,\n",
    "                \"reviews\": review_data,\n",
    "            }\n",
    "\n",
    "            # Save the extracted data to a JSON file\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the base directory exists\n",
    "base_dir = \"crawl-data-tiki\"\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)\n",
    "    \n",
    "for category in categories:\n",
    "    try:\n",
    "        extract_books_from_category(base_dir=base_dir, url=category)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Set the path to the Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "url = categories[1]\n",
    "\n",
    "# Extract the category name from the URL\n",
    "category_name = url.split(\"/\")[-2]\n",
    "\n",
    "# Ensure the base directory exists\n",
    "base_dir = \"tiki\"\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)\n",
    "\n",
    "# Create a directory for the category\n",
    "category_dir = os.path.join(base_dir, category_name)\n",
    "if not os.path.exists(category_dir):\n",
    "    os.makedirs(category_dir)\n",
    "\n",
    "driver.get(url)\n",
    "\n",
    "# Wait until the presence of at least one element with data-view-id=\"product_list_item\"\n",
    "wait = WebDriverWait(driver, 15)\n",
    "a_tags = wait.until(\n",
    "    EC.presence_of_all_elements_located(\n",
    "        (By.CSS_SELECTOR, 'a[data-view-id=\"product_list_item\"]')\n",
    "    )\n",
    ")\n",
    "print(a_tags)\n",
    "\n",
    "# Now you can proceed to extract data from the a_tags\n",
    "# For example:\n",
    "urls = []\n",
    "for a_tag in a_tags:\n",
    "    href = a_tag.get_attribute(\"href\")\n",
    "    urls.append(href)\n",
    "\n",
    "for url in urls:\n",
    "    # Load the webpage\n",
    "    driver.get(url)\n",
    "    # Wait for 5 seconds\n",
    "    time.sleep(5)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    info_data = extract_info(soup)\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Scroll to the details section\n",
    "    ActionChains(driver).scroll_by_amount(0, 2000).perform()\n",
    "    time.sleep(5)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    details = extract_details(soup)\n",
    "\n",
    "    # Scroll to the desired section\n",
    "    ActionChains(driver).scroll_by_amount(0, 1300).perform()\n",
    "    time.sleep(5)\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    rating = extract_rating(soup)\n",
    "    review_data = []\n",
    "\n",
    "    btn_next = driver.find_element(\n",
    "        By.CSS_SELECTOR,\n",
    "        \"#customer-review-widget-id > div > div.style__StyledCustomerReviews-sc-1y8vww-0.gCaHEu.customer-reviews > div > div.customer-reviews__pagination > ul > li:nth-child(7) > a\",\n",
    "    )\n",
    "\n",
    "    flag = btn_next.text\n",
    "    while btn_next:\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        data = extract_reviews(soup)\n",
    "        if data == []:\n",
    "            break\n",
    "        review_data.extend(data)\n",
    "        actions = ActionChains(driver)\n",
    "        actions.move_to_element(btn_next)\n",
    "        actions.click(btn_next)\n",
    "        actions.perform()\n",
    "        time.sleep(3)\n",
    "\n",
    "    # Get the title\n",
    "    title = info_data[\"title\"]\n",
    "\n",
    "    # Generate filename\n",
    "    filename = os.path.join(category_dir, f\"{title}.json\")\n",
    "    data = {\n",
    "        \"info\": info_data,\n",
    "        \"rating\": rating,\n",
    "        \"details\": details,\n",
    "        \"reviews\": review_data,\n",
    "    }\n",
    "\n",
    "    # Save the extracted data to a JSON file\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the webpage\n",
    "driver.get(urls[0])\n",
    "\n",
    "# Wait for 5 seconds\n",
    "time.sleep(5)\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "info_data = extract_info(soup)\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "# Scroll to the details section\n",
    "ActionChains(driver).scroll_by_amount(0, 2000).perform()\n",
    "time.sleep(5)\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "details = extract_details(soup)\n",
    "\n",
    "\n",
    "# Scroll to the desired section\n",
    "ActionChains(driver).scroll_by_amount(0, 1300).perform()\n",
    "time.sleep(5)\n",
    "soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "rating = extract_rating(soup)\n",
    "review_data = []\n",
    "\n",
    "btn_next = driver.find_element(\n",
    "    By.CSS_SELECTOR,\n",
    "    \"#customer-review-widget-id > div > div.style__StyledCustomerReviews-sc-1y8vww-0.gCaHEu.customer-reviews > div > div.customer-reviews__pagination > ul > li:nth-child(7) > a\",\n",
    ")\n",
    "\n",
    "flag = btn_next.text\n",
    "while btn_next:\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    data = extract_reviews(soup)\n",
    "    if data == []:\n",
    "        break\n",
    "    review_data.extend(data)\n",
    "    actions = ActionChains(driver)\n",
    "    actions.move_to_element(btn_next)\n",
    "    actions.click(btn_next)\n",
    "    actions.perform()\n",
    "    time.sleep(3)\n",
    "\n",
    "# Get the title\n",
    "title = info_data[\"title\"]\n",
    "\n",
    "# Generate filename\n",
    "filename = os.path.join(category_dir, f\"{title}.json\")\n",
    "data = {\"info\": info_data, \"rating\": rating, \"details\": details, \"reviews\": review_data}\n",
    "\n",
    "\n",
    "\n",
    "# Save the extracted data to a JSON file\n",
    "with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
